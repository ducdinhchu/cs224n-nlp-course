Welcome back. In the previous task, we briefly touched upon tokens where we spoke about the way ChatGPT actually works. In this task will take a deeper dive into tokens and start thinking about how that affects pricing. So we have two objectives that we actually want to overcome here. The first one is actually to define what tokens are and to illustrate how text is divided into tokens, and the second one really is to understand the ChatGPT pricing structure and how it actually relates to the number of tokens that are used. Let's get started. What exactly is a token? Well, in the context of ChatGPT, a token can be, as we said before, as short as one character or as long as one word. For example, A is a single token and Apple is also a single token, but at the same time not all words are single tokens and some words might be split into multiple tokens. It's also worth noting that punctuation marks and special characters also counter this token. If you're interacting with ChatGPT, the number of tokens you use includes the tokens in your message that you actually sent off to the API plus the tokens and the message you receive from the API. There's also something called a maximum limits, and if you come over on the right-hand side here and just hover this maximum length parameter here, we can see it says, the maximum number of tokens to degenerate requests can use up to 2,048 or 4,000 tokens shared between prompts and completion. The exact limit varies by modal. One token is roughly four characters for normal English text. So that we have rough idea, and so we do have this ability to cap the maximum length of the total prompt plus response tokens within that particular request, and we do have this handy slider here, which goes all way up to the maximum, again, which is 4,000. To have that around 250, 256, which can be usually the average maximum length.

Let's see this now as a demonstration. If we now click into our box here on the playground here, we can see at the bottom this Number 0, and as we start typing again, if we say once upon a time, you can see that spinning and it will actually create a number here. If you hover over it, there's four tokens total in prompts up to 256 tokens in response. If we wanted to maybe give this a little bit more context, we can say once upon a time there was a goblin

with an insidious

past. This is a total of 14 tokens. If we hit "Submit Now", we get our response, but it will never be able to exceed our maximum limit here.

We'll let it come to a conclusion here. We can see a new calculation of 243 tokens in the prompts and the response altogether. That's well within our limit of 256 that we actually set here. Now, let's talk about pricing. The cost of using the ChatGPT API is directly related to the number of tokens used inside of your API call. The more tokens you used the more it costs, which is a pretty simple mechanism. We can go a little bit deeper here if you click on the "Personal" tab up here, and I go over to manage accounts, it's going to get rid of everything here. I want you to go ahead and click on the "Usage" tab here. Inside of here, we'll be able to see your usage snapshot. We'll be able to usage for this month of the total amount of credits that are actually used, and this will give you a dollar amount, but what's actually quite interesting, you can actually go ahead and get a daily usage breakdown. I can go ahead and click on the "11th of June" here. Once this actually loads up, we'll be able to see two options, language model usage as well as fine-tune training. If I go ahead and click the top option here, which is the one we're actually using, and we can see a time-bound of the requests, and once we click into it, we could go ahead and see a breakdown of what was actually used and how many tokens in total were actually used and at what time. As you can see, this gives us a lot of helpful information to actually manage our tokens effectively, and as you're using GPT, whether we're in the playground and what the API, managing your tokens effectively is an important part of using ChatGPT efficiently, cost-effectively. Awesome work. We were able to actually define what tokens are and illustrate how text is actually divided into tokens, and then we went a bit further to understand the GPT pricing structure and how it relates to the amount of tokens that are actually used. In the next task, we're going to start jumping into some of the ChatGPT parameters, including model, and see how changing some of those parameters influences the outputs of those models. I look forward to seeing you there.
